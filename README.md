# Web Crawler using breadth depth search

 Web crawling is the task of investigating a collection of pages on a deep Web site that are generated dynamically in response to a specific query submitted into a search form. 
Crawlers must have features to go beyond just tracking links, such as the ability to automatically identify search terms that serve as gateways. 

The implemented system is deployed in the University of North Dakota server. We started the implementation of the GUI windows using Angular
web-framework and the database using MySQL which allows you to access and store all the user information. 
This procedure is implemented through Angular web framework. This framework is a front end web development platform which is based on a JavaScript framework. 
